{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目標\n",
    "* 資料處理前置作業<br/>\n",
    "* 創建詞袋模型(Bag of Words model)<br/>\n",
    "* 使用Naive Bayes 建置模型<br/>\n",
    "* 預測y並且產生Confusion Matrix<br/>\n",
    "* TF-IDF<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('/Users/bicc/Documents/kyo/Natural_Language_Processing/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Liked\n",
      "0                             Wow... Loved this place.      1\n",
      "1                                   Crust is not good.      0\n",
      "2            Not tasty and the texture was just nasty.      0\n",
      "3    Stopped by during the late May bank holiday of...      1\n",
      "4    The selection on the menu was great and so wer...      1\n",
      "5       Now I am getting angry and I want my damn pho.      0\n",
      "6                Honeslty it didn't taste THAT fresh.)      0\n",
      "7    The potatoes were like rubber and you could te...      0\n",
      "8                            The fries were great too.      1\n",
      "9                                       A great touch.      1\n",
      "10                            Service was very prompt.      1\n",
      "11                                  Would not go back.      0\n",
      "12   The cashier had no care what so ever on what I...      0\n",
      "13   I tried the Cape Cod ravoli, chicken, with cra...      1\n",
      "14   I was disgusted because I was pretty sure that...      0\n",
      "15   I was shocked because no signs indicate cash o...      0\n",
      "16                                 Highly recommended.      1\n",
      "17              Waitress was a little slow in service.      0\n",
      "18   This place is not worth your time, let alone V...      0\n",
      "19                                did not like at all.      0\n",
      "20                                 The Burrittos Blah!      0\n",
      "21                                  The food, amazing.      1\n",
      "22                               Service is also cute.      1\n",
      "23   I could care less... The interior is just beau...      1\n",
      "24                                  So they performed.      1\n",
      "25   That's right....the red velvet cake.....ohhh t...      1\n",
      "26          - They never brought a salad we asked for.      0\n",
      "27   This hole in the wall has great Mexican street...      1\n",
      "28   Took an hour to get our food only 4 tables in ...      0\n",
      "29                   The worst was the salmon sashimi.      0\n",
      "..                                                 ...    ...\n",
      "970  I immediately said I wanted to talk to the man...      0\n",
      "971                    The ambiance isn't much better.      0\n",
      "972  Unfortunately, it only set us up for disapppoi...      0\n",
      "973                              The food wasn't good.      0\n",
      "974  Your servers suck, wait, correction, our serve...      0\n",
      "975      What happened next was pretty....off putting.      0\n",
      "976  too bad cause I know it's family owned, I real...      0\n",
      "977               Overpriced for what you are getting.      0\n",
      "978               I vomited in the bathroom mid lunch.      0\n",
      "979  I kept looking at the time and it had soon bec...      0\n",
      "980  I have been to very few places to eat that und...      0\n",
      "981  We started with the tuna sashimi which was bro...      0\n",
      "982                            Food was below average.      0\n",
      "983  It sure does beat the nachos at the movies but...      0\n",
      "984       All in all, Ha Long Bay was a bit of a flop.      0\n",
      "985  The problem I have is that they charge $11.99 ...      0\n",
      "986  Shrimp- When I unwrapped it (I live only 1/2 a...      0\n",
      "987     It lacked flavor, seemed undercooked, and dry.      0\n",
      "988  It really is impressive that the place hasn't ...      0\n",
      "989  I would avoid this place if you are staying in...      0\n",
      "990  The refried beans that came with my meal were ...      0\n",
      "991         Spend your money and time some place else.      0\n",
      "992  A lady at the table next to us found a live gr...      0\n",
      "993            the presentation of the food was awful.      0\n",
      "994           I can't tell you how disappointed I was.      0\n",
      "995  I think food should have flavor and texture an...      0\n",
      "996                           Appetite instantly gone.      0\n",
      "997  Overall I was not impressed and would not go b...      0\n",
      "998  The whole experience was underwhelming, and I ...      0\n",
      "999  Then, as if I hadn't wasted enough of my life ...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 引入正規表示套件(re)<br/>\n",
    "* 引入自然語言工具包(NLTK)：<br/>\n",
    " nltk.corpus:獲取和處理語料庫<br/>\n",
    " nltk.stem.porter.PorterStemmer:從英文單字中獲得符合語法的詞幹<br/>\n",
    "* re.sub各參數解說：<br/>\n",
    "re.sub(pattern, repl, string, count=0, flags=0)<br/>\n",
    ">pattern:正則中的模式字符串<br/>\n",
    "repl:被替換的字符串<br/>\n",
    "string:欲處理的字符串<br/>\n",
    "count:指定處理部分內容<br/>\n",
    "flags:<br/>\n",
    "* PorterStemmer( ).stem:提取詞幹<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/bicc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分段執行範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honeslty it didn t taste that fresh  \n",
      "['honeslty', 'it', 'didn', 't', 'taste', 'that', 'fresh']\n",
      "['honeslti', 'it', 'didn', 't', 'tast', 'that', 'fresh']\n",
      "['honeslti', 'tast', 'fresh']\n",
      "honeslti tast fresh\n"
     ]
    }
   ],
   "source": [
    "review1 = re.sub('[^a-zA-Z]', ' ', dataset['Review'][6])\n",
    "review2 = review1.lower()\n",
    "review3 = review2.split()\n",
    "ps = PorterStemmer()\n",
    "review4 = [ps.stem(word) for word in review3]\n",
    "review5 = [ps.stem(word) for word in review4 if not word in set(stopwords.words('english'))]\n",
    "review6 = ' '.join(review5)\n",
    "print(review2)\n",
    "print(review3)\n",
    "print(review4)\n",
    "print(review5)\n",
    "print(review6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(sklearn.base.BaseEstimator, VectorizerMixin)\n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.coo_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  lowercase : boolean, True by default\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * TF（Term Frequency）: 表示某個關鍵詞在整篇文章中出現的頻率<br/>\n",
    "#### * CountVectorizer : 將本文中的詞語轉換為詞頻矩陣<br/>\n",
    "以下介紹CountVectorizer內的參數：<br/>\n",
    "* strip_accents : {‘ascii’, ‘unicode’, None}<br/>\n",
    "* lowercase : boolean, True by default：計算前，先將所有字符轉化為小寫。這個參數一般為True。<br/>\n",
    "* preprocessor : callable or None (default)<br/>\n",
    "* tokenizer : callable or None (default)<br/>\n",
    "* stop_words : string {‘english’}, list, or None (default)<br/>\n",
    "* token_pattern : string：正則表達式，默認篩選長度大於等於2的字母和數字混合字符，參數analyzer設置為word時才有效。<br/>\n",
    "* ngram_range : tuple (min_n, max_n)：n-values值得上下界，默認是ngram_range=(1, 1)，該範圍之內的n元feature都會被提取出來，根據自己的需求調整。<br/>\n",
    "* analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable：關鍵詞基於wordn-grams還是character n-grams。如果是callable是自己複寫的從the raw, unprocessed input提取關鍵詞的函數。<br/>\n",
    "* max_df : float in range [0.0, 1.0] or int, default=1.0<br/>\n",
    "* min_df : float in range [0.0, 1.0] or int, default=1：按比例，或絕對數量刪除df超過max_df或者df小於min_df的word tokens。有效的前提是參數vocabulary設置成Node。<br/>\n",
    "* max_features : int or None, default=None：選擇tf最大的max_features個關鍵詞。有效的前提是參數vocabulary設置成Node。<br/>\n",
    "* vocabulary : Mapping or iterable, optional：自定義的word tokens，如果不是None，則只計算vocabulary中關鍵詞的tf。還是設為None靠譜。<br/>\n",
    "* binary : boolean, default=False：如果是True，tf的值只有0和1，表示出現和不出現。<br/>\n",
    "* dtype : type, optional：Type of the matrix returned by fit_transform() or transform().<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wow': 1482, 'love': 777, 'place': 963, 'crust': 310, 'good': 569, 'tasti': 1297, 'textur': 1309, 'nasti': 870, 'stop': 1246, 'late': 737, 'may': 809, 'bank': 92, 'holiday': 642, 'rick': 1084, 'steve': 1239, 'recommend': 1050, 'select': 1140, 'menu': 827, 'great': 583, 'price': 997, 'get': 553, 'angri': 33, 'want': 1432, 'damn': 319, 'pho': 951, 'honeslti': 645, 'tast': 1295, 'fresh': 528, 'potato': 989, 'like': 760, 'rubber': 1098, 'could': 283, 'tell': 1302, 'made': 788, 'ahead': 15, 'time': 1331, 'kept': 720, 'warmer': 1434, 'fri': 529, 'touch': 1349, 'servic': 1149, 'prompt': 1009, 'would': 1480, 'go': 563, 'back': 83, 'cashier': 204, 'care': 196, 'ever': 447, 'say': 1125, 'still': 1241, 'end': 431, 'wayyy': 1442, 'overpr': 916, 'tri': 1359, 'cape': 192, 'cod': 245, 'ravoli': 1040, 'chicken': 222, 'cranberri': 296, 'mmmm': 847, 'disgust': 370, 'pretti': 996, 'sure': 1281, 'human': 662, 'hair': 603, 'shock': 1158, 'sign': 1169, 'indic': 684, 'cash': 202, 'highli': 635, 'waitress': 1429, 'littl': 766, 'slow': 1182, 'worth': 1479, 'let': 752, 'alon': 23, 'vega': 1403, 'burritto': 173, 'food': 514, 'amaz': 27, 'also': 24, 'cute': 317, 'less': 751, 'interior': 698, 'beauti': 108, 'perform': 943, 'right': 1086, 'red': 1051, 'velvet': 1409, 'cake': 183, 'stuff': 1259, 'never': 879, 'brought': 159, 'salad': 1109, 'ask': 59, 'hole': 641, 'wall': 1431, 'mexican': 831, 'street': 1251, 'taco': 1287, 'friendli': 532, 'staff': 1227, 'took': 1343, 'hour': 657, 'tabl': 1286, 'restaur': 1074, 'luke': 783, 'warm': 1433, 'sever': 1151, 'run': 1100, 'around': 55, 'total': 1348, 'overwhelm': 917, 'worst': 1478, 'salmon': 1110, 'sashimi': 1118, 'combo': 252, 'burger': 171, 'beer': 111, 'decent': 326, 'deal': 325, 'final': 491, 'blow': 135, 'found': 521, 'accid': 2, 'happier': 614, 'seem': 1138, 'quick': 1027, 'grab': 576, 'bite': 130, 'familiar': 473, 'pub': 1014, 'favor': 479, 'look': 772, 'elsewher': 427, 'overal': 914, 'lot': 775, 'redeem': 1052, 'qualiti': 1025, 'inexpens': 687, 'ampl': 31, 'portion': 985, 'poor': 981, 'waiter': 1428, 'feel': 481, 'stupid': 1260, 'everi': 448, 'came': 187, 'first': 497, 'visit': 1420, 'hiro': 639, 'delight': 341, 'suck': 1267, 'shrimp': 1166, 'tender': 1305, 'moist': 849, 'enough': 434, 'drag': 390, 'establish': 441, 'hard': 615, 'judg': 716, 'whether': 1456, 'side': 1168, 'gross': 593, 'melt': 824, 'styrofoam': 1262, 'eat': 410, 'sick': 1167, 'posit': 986, 'note': 893, 'server': 1148, 'attent': 66, 'provid': 1013, 'frozen': 534, 'puck': 1017, 'peopl': 939, 'behind': 113, 'regist': 1059, 'thing': 1317, 'prime': 999, 'rib': 1080, 'dessert': 352, 'section': 1136, 'bad': 85, 'gener': 551, 'beef': 110, 'cook': 275, 'sandwich': 1116, 'firehous': 496, 'greek': 586, 'dress': 395, 'pita': 961, 'hummu': 664, 'refresh': 1056, 'order': 909, 'duck': 403, 'rare': 1034, 'pink': 959, 'insid': 692, 'nice': 882, 'char': 209, 'outsid': 911, 'us': 1394, 'realiz': 1044, 'husband': 668, 'left': 746, 'sunglass': 1278, 'chow': 230, 'mein': 822, 'horribl': 651, 'attitud': 67, 'toward': 1351, 'custom': 315, 'talk': 1291, 'one': 906, 'enjoy': 433, 'huge': 661, 'wonder': 1470, 'imagin': 675, 'heart': 624, 'attack': 65, 'grill': 589, 'downtown': 389, 'absolut': 0, 'flat': 501, 'line': 763, 'excus': 457, 'much': 862, 'seafood': 1131, 'string': 1254, 'pasta': 928, 'bottom': 144, 'amount': 30, 'sauc': 1122, 'power': 992, 'scallop': 1126, 'perfectli': 942, 'rip': 1089, 'banana': 91, 'petrifi': 947, 'tasteless': 1296, 'least': 743, 'think': 1318, 'refil': 1054, 'water': 1439, 'struggl': 1257, 'wave': 1440, 'minut': 842, 'receiv': 1048, 'star': 1230, 'appet': 48, 'cocktail': 243, 'handmad': 610, 'delici': 339, 'definit': 335, 'glad': 558, 'give': 556, 'militari': 837, 'discount': 368, 'alway': 26, 'do': 375, 'gringo': 590, 'updat': 1391, 'went': 1452, 'second': 1135, 'got': 573, 'appar': 46, 'heard': 623, 'salt': 1112, 'batter': 102, 'fish': 498, 'chewi': 221, 'way': 1441, 'finish': 494, 'includ': 680, 'drink': 398, 'jeff': 706, 'beyond': 120, 'expect': 459, 'realli': 1045, 'rice': 1082, 'meh': 821, 'min': 840, 'milkshak': 839, 'noth': 894, 'chocol': 227, 'milk': 838, 'guess': 598, 'known': 729, 'excalibur': 453, 'use': 1395, 'common': 255, 'sens': 1143, 'dish': 371, 'quit': 1029, 'appal': 45, 'valu': 1400, 'well': 1451, 'sweet': 1284, 'season': 1133, 'today': 1336, 'lunch': 785, 'buffet': 166, 'cheat': 214, 'wast': 1437, 'opportun': 907, 'compani': 256, 'come': 253, 'experienc': 462, 'underwhelm': 1378, 'relationship': 1063, 'parti': 926, 'wait': 1427, 'person': 946, 'break': 152, 'walk': 1430, 'smell': 1187, 'old': 904, 'greas': 581, 'trap': 1357, 'other': 910, 'turkey': 1369, 'roast': 1091, 'bland': 132, 'everyon': 449, 'rave': 1039, 'sugari': 1271, 'disast': 366, 'tailor': 1288, 'six': 1178, 'year': 1490, 'spring': 1225, 'roll': 1093, 'oh': 902, 'yummi': 1498, 'meat': 814, 'ratio': 1038, 'unsatisfi': 1387, 'die': 355, 'everyth': 450, 'summari': 1274, 'larg': 733, 'disappoint': 364, 'dine': 358, 'experi': 461, 'sexi': 1153, 'mouth': 858, 'flirt': 506, 'hottest': 656, 'rock': 1092, 'casino': 205, 'step': 1238, 'forward': 520, 'best': 118, 'breakfast': 153, 'bye': 178, 'tip': 1333, 'ladi': 732, 'arriv': 57, 'quickli': 1028, 'cafe': 182, 'serv': 1147, 'fantast': 475, 'wife': 1461, 'garlic': 547, 'bone': 139, 'marrow': 806, 'ad': 11, 'extra': 466, 'meal': 812, 'anoth': 35, 'help': 631, 'bloddi': 133, 'mari': 803, 'town': 1352, 'cannot': 190, 'beat': 106, 'wine': 1463, 'reduct': 1053, 'better': 119, 'tigerlilli': 1330, 'afternoon': 13, 'bartend': 96, 'ambienc': 29, 'music': 867, 'play': 970, 'next': 881, 'trip': 1362, 'sooooo': 1204, 'real': 1043, 'sushi': 1283, 'lover': 778, 'honest': 646, 'yama': 1487, 'pass': 927, 'busi': 174, 'thai': 1311, 'spici': 1219, 'check': 215, 'atmospher': 62, 'kind': 726, 'mess': 829, 'steak': 1235, 'although': 25, 'sound': 1208, 'actual': 10, 'bit': 129, 'know': 728, 'manag': 798, 'eaten': 411, 'prepar': 994, 'indian': 683, 'cuisin': 313, 'boot': 141, 'worri': 1476, 'fine': 493, 'guy': 600, 'son': 1200, 'said': 1107, 'thought': 1324, 'ventur': 1411, 'away': 74, 'hit': 640, 'spot': 1223, 'night': 884, 'host': 653, 'lack': 731, 'word': 1472, 'number': 896, 'reason': 1046, 'review': 1076, 'leav': 745, 'phenomen': 949, 'ambianc': 28, 'return': 1075, 'strip': 1255, 'pork': 984, 'belli': 116, 'mediocr': 817, 'penn': 938, 'vodka': 1421, 'excel': 455, 'massiv': 808, 'meatloaf': 816, 'crispi': 305, 'wrap': 1483, 'delish': 342, 'tuna': 1368, 'rude': 1099, 'nyc': 899, 'bagel': 86, 'cream': 300, 'chees': 217, 'lox': 780, 'caper': 193, 'even': 445, 'subway': 1265, 'fact': 469, 'meet': 820, 'serious': 1145, 'solid': 1193, 'bar': 93, 'extrem': 467, 'mani': 801, 'weekend': 1447, 'empti': 430, 'suggest': 1272, 'ate': 61, 'curri': 314, 'bamboo': 90, 'shoot': 1160, 'moz': 860, 'top': 1344, 'done': 378, 'cover': 290, 'subpar': 1264, 'bathroom': 101, 'clean': 238, 'decor': 329, 'chang': 208, 'consid': 269, 'pace': 919, 'thumb': 1329, 'watch': 1438, 'pay': 931, 'ignor': 673, 'fianc': 486, 'middl': 834, 'day': 323, 'greet': 588, 'seat': 1134, 'mandalay': 799, 'bay': 103, 'forti': 519, 'five': 499, 'vain': 1398, 'crostini': 306, 'stale': 1228, 'highlight': 636, 'nigiri': 885, 'joint': 713, 'differ': 356, 'cut': 316, 'piec': 955, 'flavor': 502, 'voodoo': 1423, 'sinc': 1175, 'gluten': 562, 'free': 525, 'ago': 14, 'unfortun': 1380, 'must': 868, 'bakeri': 87, 'leftov': 747, 'reloc': 1066, 'impress': 678, 'immedi': 676, 'divers': 374, 'avoid': 72, 'cost': 280, 'full': 539, 'hand': 608, 'phoenix': 952, 'metro': 830, 'area': 52, 'treat': 1358, 'bacon': 84, 'hella': 629, 'salti': 1113, 'spinach': 1221, 'avocado': 71, 'ingredi': 690, 'sad': 1103, 'liter': 765, 'zero': 1499, 'list': 764, 'lordi': 773, 'khao': 722, 'soi': 1192, 'miss': 844, 'terrif': 1308, 'thrill': 1326, 'accommod': 3, 'vegetarian': 1406, 'daughter': 322, 'perhap': 944, 'inspir': 693, 'desir': 349, 'modern': 848, 'hip': 638, 'maintain': 794, 'cozi': 292, 'weekli': 1448, 'haunt': 619, 'sat': 1119, 'take': 1289, 'overcook': 915, 'charcoal': 210, 'decid': 327, 'send': 1142, 'verg': 1413, 'probabl': 1002, 'dirt': 361, 'someth': 1197, 'healthi': 622, 'quantiti': 1026, 'lemon': 750, 'raspberri': 1035, 'ice': 671, 'incred': 682, 'interest': 697, 'crepe': 303, 'station': 1233, 'hot': 655, 'bread': 151, 'butter': 176, 'home': 643, 'chip': 224, 'egg': 421, 'gyro': 601, 'wing': 1464, 'satisfi': 1121, 'joey': 711, 'vote': 1424, 'dog': 376, 'valley': 1399, 'reader': 1042, 'magazin': 791, 'bowl': 147, 'live': 767, 'friday': 530, 'insult': 696, 'felt': 485, 'disrespect': 373, 'drive': 400, 'exceed': 454, 'hope': 650, 'dream': 393, 'serivc': 1146, 'brunch': 161, 'invit': 699, 'last': 735, 'foot': 515, 'mix': 846, 'mushroom': 866, 'yukon': 1496, 'gold': 566, 'pure': 1021, 'white': 1457, 'corn': 277, 'beateou': 107, 'bug': 167, 'show': 1164, 'given': 557, 'climb': 239, 'kitchen': 727, 'soon': 1202, 'friend': 531, 'tartar': 1294, 'though': 1323, 'soggi': 1191, 'jamaican': 704, 'mojito': 850, 'small': 1183, 'rich': 1083, 'accordingli': 5, 'shower': 1165, 'rins': 1088, 'unless': 1384, 'mind': 841, 'nude': 895, 'see': 1137, 'lobster': 768, 'bisqu': 128, 'bussel': 175, 'sprout': 1226, 'risotto': 1090, 'filet': 488, 'need': 874, 'pepper': 940, 'cours': 287, 'none': 890, 'someon': 1196, 'either': 423, 'cold': 247, 'date': 321, 'unbeliev': 1375, 'bargain': 95, 'folk': 512, 'make': 795, 'welcom': 1450, 'special': 1215, 'main': 793, 'uninspir': 1382, 'whenev': 1455, 'world': 1475, 'annoy': 34, 'drunk': 402, 'fun': 540, 'chef': 220, 'doubl': 382, 'cheeseburg': 218, 'singl': 1176, 'patti': 930, 'apart': 42, 'pictur': 954, 'upload': 1393, 'yeah': 1489, 'coupl': 285, 'sport': 1222, 'event': 446, 'tv': 1371, 'possibl': 987, 'descript': 347, 'yum': 1497, 'eel': 417, 'yet': 1494, 'mayo': 811, 'hardest': 616, 'decis': 328, 'honestli': 647, 'suppos': 1280, 'eye': 468, 'stay': 1234, 'money': 852, 'flavour': 504, 'almost': 22, 'build': 168, 'freez': 526, 'close': 240, 'point': 977, 'ayc': 78, 'light': 757, 'dark': 320, 'set': 1150, 'mood': 855, 'base': 97, 'sub': 1263, 'par': 924, 'effort': 420, 'gratitud': 579, 'owner': 918, 'privileg': 1000, 'work': 1473, 'creami': 301, 'similar': 1171, 'complaint': 259, 'silent': 1170, 'pizza': 962, 'peanut': 935, 'fast': 478, 'godfath': 565, 'tough': 1350, 'short': 1162, 'stick': 1240, 'recal': 1047, 'charg': 211, 'tap': 1292, 'exquisit': 464, 'plu': 975, 'buck': 165, 'thu': 1328, 'far': 476, 'twice': 1372, 'self': 1141, 'proclaim': 1004, 'coffe': 246, 'wildli': 1462, 'veggitarian': 1408, 'platter': 969, 'cant': 191, 'wrong': 1485, 'madison': 790, 'ironman': 700, 'job': 710, 'dedic': 330, 'boba': 138, 'tea': 1299, 'jenni': 707, 'patio': 929, 'outstand': 912, 'goat': 564, 'skimp': 1179, 'mac': 786, 'bachi': 82, 'stink': 1242, 'burn': 172, 'saganaki': 1106, 'hate': 618, 'disagre': 363, 'fellow': 484, 'yelper': 1493, 'later': 738, 'neighborhood': 877, 'conveni': 274, 'locat': 769, 'pull': 1018, 'soooo': 1203, 'gave': 548, 'rate': 1036, 'pleas': 971, 'third': 1320, 'write': 1484, 'stir': 1243, 'noodl': 891, 'count': 284, 'box': 148, 'bore': 142, 'greedi': 585, 'corpor': 278, 'dime': 357, 'atroci': 63, 'summer': 1275, 'charm': 212, 'toast': 1335, 'english': 432, 'muffin': 863, 'untoast': 1388, 'high': 634, 'hous': 658, 'bu': 164, 'boy': 149, 'basic': 99, 'figur': 487, 'joke': 714, 'publicli': 1016, 'loudli': 776, 'bbq': 104, 'lighter': 758, 'fare': 477, 'public': 1015, 'two': 1373, 'happi': 613, 'downsid': 388, 'without': 1469, 'doubt': 383, 'except': 456, 'month': 854, 'favorit': 480, 'shawarrrrrrma': 1156, 'black': 131, 'pea': 933, 'unreal': 1386, 'vinaigrett': 1417, 'seen': 1139, 'especi': 440, 'mom': 851, 'pleasant': 972, 'honor': 648, 'hut': 669, 'coupon': 286, 'truli': 1365, 'dirti': 362, 'replenish': 1069, 'plain': 964, 'yucki': 1495, 'standard': 1229, 'omg': 905, 'delicioso': 340, 'authent': 69, 'spaghetti': 1214, 'whatsoev': 1453, 'veget': 1405, 'tucson': 1366, 'chipotl': 226, 'classi': 236, 'succul': 1266, 'basebal': 98, 'brick': 155, 'oven': 913, 'app': 44, 'multipl': 865, 'ten': 1304, 'terribl': 1307, 'equal': 439, 'pancak': 922, 'genuin': 552, 'enthusiast': 436, 'sadli': 1104, 'gordon': 572, 'ramsey': 1030, 'shall': 1154, 'sharpli': 1155, 'life': 756, 'door': 381, 'offer': 901, 'cool': 276, 'turn': 1370, 'els': 426, 'buy': 177, 'handl': 609, 'rowdi': 1097, 'find': 492, 'despic': 350, 'soup': 1210, 'lukewarm': 784, 'crave': 297, 'deserv': 348, 'stomach': 1244, 'ach': 7, 'rest': 1072, 'drop': 401, 'ball': 89, 'space': 1213, 'tini': 1332, 'elegantli': 424, 'comfort': 254, 'usual': 1396, 'eggplant': 422, 'green': 587, 'bean': 105, 'part': 925, 'inconsider': 681, 'hi': 633, 'dinner': 359, 'halibut': 605, 'told': 1338, 'happen': 612, 'car': 194, 'front': 533, 'starv': 1232, 'disgrac': 369, 'def': 333, 'ethic': 443, 'continu': 273, 'andddd': 32, 'anyon': 38, 'stuf': 1258, 'crystal': 312, 'shop': 1161, 'mall': 796, 'aria': 54, 'summar': 1273, 'nay': 871, 'transcend': 1356, 'bring': 156, 'joy': 715, 'memori': 825, 'pneumat': 976, 'condiment': 266, 'dispens': 372, 'ian': 670, 'kid': 723, 'option': 908, 'kiddo': 724, 'perfect': 941, 'famili': 472, 'impecc': 677, 'simpli': 1174, 'bouchon': 145, 'account': 6, 'screw': 1130, 'remind': 1068, 'pop': 983, 'san': 1115, 'francisco': 523, 'buldogi': 169, 'gourmet': 575, 'frustrat': 536, 'petti': 948, 'hungri': 666, 'assur': 60, 'teeth': 1301, 'sore': 1206, 'complet': 260, 'becom': 109, 'regular': 1060, 'profession': 1005, 'companion': 257, 'ground': 594, 'smear': 1186, 'track': 1353, 'everywher': 451, 'pile': 956, 'bird': 126, 'poop': 980, 'furthermor': 542, 'websit': 1444, 'mistak': 845, 'expert': 463, 'connisseur': 267, 'topic': 1345, 'jerk': 708, 'strike': 1253, 'rush': 1101, 'nicest': 883, 'across': 9, 'biscuit': 127, 'absolutley': 1, 'awkward': 76, 'lb': 742, 'cow': 291, 'th': 1310, 'gristl': 591, 'steiner': 1237, 'dollar': 377, 'anyway': 41, 'fs': 537, 'week': 1446, 'mention': 826, 'combin': 251, 'pear': 936, 'almond': 21, 'big': 121, 'winner': 1465, 'spicier': 1220, 'prefer': 993, 'ribey': 1081, 'mesquit': 828, 'anytim': 40, 'gooodd': 571, 'connoisseur': 268, 'contain': 272, 'driest': 397, 'relax': 1064, 'venu': 1412, 'group': 595, 'etc': 442, 'tater': 1298, 'tot': 1347, 'southwest': 1212, 'paid': 921, 'vanilla': 1401, 'smooth': 1189, 'profiterol': 1006, 'choux': 229, 'im': 674, 'az': 79, 'new': 880, 'carli': 197, 'due': 405, 'acknowledg': 8, 'forget': 517, 'margarita': 802, 'ventil': 1410, 'upgrad': 1392, 'letdown': 753, 'rather': 1037, 'camelback': 188, 'flower': 508, 'cartel': 200, 'trim': 1361, 'claim': 234, 'bill': 124, 'jewel': 709, 'la': 730, 'exactli': 452, 'nearli': 872, 'limit': 762, 'crab': 294, 'leg': 748, 'toro': 1346, 'thinli': 1319, 'slice': 1181, 'wagyu': 1426, 'truffl': 1364, 'dont': 379, 'long': 770, 'attach': 64, 'ga': 544, 'awesom': 75, 'wors': 1477, 'humili': 663, 'worker': 1474, 'bunch': 170, 'call': 185, 'conclus': 265, 'fill': 489, 'daili': 318, 'tragedi': 1355, 'struck': 1256, 'crawfish': 298, 'monster': 853, 'funni': 541, 'multi': 864, 'grain': 577, 'pumpkin': 1019, 'pecan': 937, 'fluffi': 509, 'airlin': 16, 'noca': 888, 'lettuc': 754, 'thoroughli': 1322, 'homemad': 644, 'thin': 1316, 'cheesecurd': 219, 'typic': 1374, 'glanc': 559, 'item': 702, 'gone': 568, 'greasi': 582, 'unhealthi': 1381, 'might': 835, 'similarli': 1172, 'deliveri': 344, 'man': 797, 'apolog': 43, 'expens': 460, 'pack': 920, 'tiramisu': 1334, 'cannoli': 189, 'sun': 1276, 'whole': 1458, 'choos': 228, 'frenchman': 527, 'martini': 807, 'entre': 438, 'gc': 549, 'sampl': 1114, 'thirti': 1321, 'vacant': 1397, 'yellowtail': 1492, 'carpaccio': 198, 'stranger': 1249, 'hello': 630, 'strang': 1248, 'boyfriend': 150, 'recent': 1049, 'donut': 380, 'save': 1124, 'room': 1094, 'mayb': 810, 'howev': 659, 'suffer': 1269, 'tapa': 1293, 'vinegrett': 1418, 'babi': 81, 'believ': 114, 'hanker': 611, 'forth': 518, 'theft': 1314, 'eew': 418, 'wit': 1468, 'guest': 599, 'regularli': 1061, 'super': 1279, 'swung': 1285, 'deepli': 332, 'effici': 419, 'fan': 474, 'sucker': 1268, 'dri': 396, 'cheap': 213, 'perpar': 945, 'present': 995, 'giant': 554, 'lightli': 759, 'dust': 407, 'powder': 991, 'sugar': 1270, 'fo': 510, 'accomod': 4, 'vegan': 1404, 'veggi': 1407, 'crumbi': 309, 'color': 250, 'instead': 695, 'crouton': 307, 'crema': 302, 'caf': 181, 'expand': 458, 'wish': 1467, 'philadelphia': 950, 'sit': 1177, 'fairli': 471, 'crisp': 304, 'north': 892, 'scottsdal': 1128, 'soooooo': 1205, 'freak': 524, 'paper': 923, 'reheat': 1062, 'ok': 903, 'wedg': 1445, 'sorri': 1207, 'tongu': 1341, 'cheek': 216, 'bloodi': 134, 'despit': 351, 'yellow': 1491, 'saffron': 1105, 'thru': 1327, 'mean': 813, 'half': 604, 'somehow': 1195, 'luck': 782, 'non': 889, 'focus': 511, 'grandmoth': 578, 'hostess': 654, 'four': 522, 'blue': 137, 'shirt': 1157, 'vibe': 1416, 'drastic': 391, 'caesar': 180, 'promptli': 1010, 'madhous': 789, 'proven': 1012, 'dead': 324, 'greatest': 584, 'macaron': 787, 'insan': 691, 'inform': 689, 'somewhat': 1199, 'edibl': 414, 'promis': 1008, 'fail': 470, 'deliv': 343, 'averag': 70, 'plater': 968, 'togeth': 1337, 'poorli': 982, 'construct': 271, 'italian': 701, 'scream': 1129, 'legit': 749, 'book': 140, 'somethat': 1198, 'duo': 406, 'violinist': 1419, 'song': 1201, 'request': 1070, 'baklava': 88, 'baba': 80, 'ganoush': 545, 'mgm': 832, 'courteou': 289, 'eclect': 412, 'ring': 1087, 'nobu': 887, 'googl': 570, 'smashburg': 1185, 'gem': 550, 'plantain': 965, 'spend': 1217, 'cotta': 282, 'slaw': 1180, 'drench': 394, 'piano': 953, 'soundtrack': 1209, 'rge': 1078, 'fillet': 490, 'relleno': 1065, 'plate': 967, 'sergeant': 1144, 'auju': 68, 'hawaiian': 620, 'breez': 154, 'mango': 800, 'magic': 792, 'pineappl': 958, 'smoothi': 1190, 'mortifi': 856, 'needless': 875, 'drip': 399, 'mostli': 857, 'hospit': 652, 'industri': 686, 'refrain': 1055, 'cibo': 232, 'longer': 771, 'read': 1041, 'pro': 1001, 'simpl': 1173, 'dough': 385, 'tonight': 1342, 'elk': 425, 'hook': 649, 'classic': 237, 'quaint': 1023, 'compliment': 261, 'thank': 1312, 'dylan': 408, 'tummi': 1367, 'gratuiti': 580, 'larger': 734, 'fli': 505, 'appl': 50, 'juic': 717, 'han': 607, 'bare': 94, 'ryan': 1102, 'edinburgh': 415, 'revisit': 1077, 'chines': 223, 'pine': 957, 'nut': 897, 'airport': 17, 'speedi': 1216, 'calligraphi': 186, 'anyth': 39, 'complain': 258, 'stood': 1245, 'begin': 112, 'awkwardli': 77, 'extens': 465, 'wide': 1459, 'array': 56, 'inflat': 688, 'smaller': 1184, 'grow': 596, 'rapidli': 1033, 'lil': 761, 'fuzzi': 543, 'wonton': 1471, 'thick': 1315, 'level': 755, 'spice': 1218, 'whelm': 1454, 'crowd': 308, 'mid': 833, 'arepa': 53, 'jalapeno': 703, 'that': 1313, 'shoe': 1159, 'leather': 744, 'defin': 334, 'low': 779, 'key': 721, 'afford': 12, 'sour': 1211, 'sunday': 1277, 'tradit': 1354, 'hunan': 665, 'style': 1261, 'bother': 143, 'flair': 500, 'nutshel': 898, 'restaraunt': 1073, 'market': 805, 'sewer': 1152, 'girlfriend': 555, 'veal': 1402, 'satifi': 1120, 'join': 712, 'club': 241, 'via': 1415, 'email': 428, 'case': 201, 'colder': 248, 'flavorless': 503, 'describ': 346, 'tepid': 1306, 'chain': 207, 'easili': 409, 'nacho': 869, 'crazi': 299, 'juri': 718, 'lawyer': 741, 'court': 288, 'wienerschnitzel': 1460, 'idea': 672, 'brother': 158, 'law': 740, 'herea': 632, 'tribut': 1360, 'held': 627, 'salsa': 1111, 'pissd': 960, 'surpris': 1282, 'golden': 567, 'fell': 482, 'bruschetta': 162, 'devin': 354, 'employe': 429, 'lastli': 736, 'mozzarella': 861, 'neglig': 876, 'unwelcom': 1389, 'consist': 270, 'fruit': 535, 'peach': 934, 'blown': 136, 'put': 1022, 'plastic': 966, 'cram': 295, 'takeout': 1290, 'cr': 293, 'pe': 932, 'delic': 338, 'aw': 73, 'kabuki': 719, 'maria': 804, 'articl': 58, 'fuck': 538, 'caballero': 179, 'head': 621, 'round': 1096, 'disbelief': 367, 'qualifi': 1024, 'version': 1414, 'toler': 1339, 'polit': 979, 'wash': 1436, 'heat': 625, 'coconut': 244, 'fella': 483, 'huevo': 660, 'ranchero': 1032, 'appeal': 47, 'pricey': 998, 'temp': 1303, 'glove': 561, 'deep': 331, 'pleasur': 973, 'plethora': 974, 'seal': 1132, 'approv': 51, 'colleg': 249, 'class': 235, 'start': 1231, 'edit': 416, 'besid': 117, 'costco': 281, 'uniqu': 1383, 'weird': 1449, 'hardli': 617, 'groceri': 592, 'store': 1247, 'japanes': 705, 'dude': 404, 'doughi': 386, 'inch': 679, 'wire': 1466, 'albondiga': 19, 'tomato': 1340, 'meatbal': 815, 'three': 1325, 'medium': 819, 'refus': 1058, 'anymor': 37, 'killer': 725, 'latt': 739, 'allergi': 20, 'warn': 1435, 'clue': 242, 'mediterranean': 818, 'rotat': 1095, 'concern': 264, 'mellow': 823, 'strawberri': 1250, 'unprofession': 1385, 'loyal': 781, 'bellagio': 115, 'anticip': 36, 'weak': 1443, 'correct': 279, 'bought': 146, 'sal': 1108, 'unexperienc': 1379, 'steakhous': 1236, 'properli': 1011, 'understand': 1377, 'concept': 263, 'guacamol': 597, 'pur': 1020, 'ed': 413, 'postino': 988, 'poison': 978, 'batch': 100, 'yay': 1488, 'hilari': 637, 'christma': 231, 'eve': 444, 'rememb': 1067, 'biggest': 123, 'entir': 437, 'teamwork': 1300, 'degre': 336, 'ri': 1079, 'calamari': 184, 'fondu': 513, 'lost': 774, 'forev': 516, 'scene': 1127, 'denni': 345, 'downright': 387, 'waaaaaayyyyyyyyyi': 1425, 'sangria': 1117, 'glass': 560, 'ridicul': 1085, 'brisket': 157, 'neat': 873, 'trippi': 1363, 'hurri': 667, 'reserv': 1071, 'stretch': 1252, 'cashew': 203, 'undercook': 1376, 'chipolt': 225, 'ranch': 1031, 'dip': 360, 'saus': 1123, 'douchey': 384, 'indoor': 685, 'garden': 546, 'con': 262, 'spotti': 1224, 'neither': 878, 'ensu': 435, 'bing': 125, 'carb': 195, 'profound': 1007, 'deuchebaggeri': 353, 'smoke': 1188, 'solidifi': 1194, 'ala': 18, 'cart': 199, 'del': 337, 'hamburg': 606, 'hell': 628, 'gotten': 574, 'ya': 1486, 'shot': 1163, 'firebal': 495, 'disapppoint': 365, 'heimer': 626, 'vomit': 1422, 'circumst': 233, 'brownish': 160, 'obvious': 900, 'movi': 859, 'ha': 602, 'flop': 507, 'problem': 1003, 'bigger': 122, 'unwrap': 1390, 'mile': 836, 'brushfir': 163, 'mirag': 843, 'refri': 1057, 'crusti': 311, 'caterpillar': 206, 'appetit': 49, 'instantli': 694, 'ninja': 886, 'pour': 990, 'wound': 1481, 'draw': 392}\n"
     ]
    }
   ],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切割資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liouscott/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Naive Bayes 建置模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測y並且產生Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.43370786  0.          0.55847784  0.55847784  0.\n",
      "   0.43370786  0.          0.        ]\n",
      " [ 0.          0.43370786  0.          0.          0.          0.55847784\n",
      "   0.43370786  0.          0.55847784]\n",
      " [ 0.50238645  0.44507629  0.50238645  0.19103892  0.19103892  0.19103892\n",
      "   0.29671753  0.25119322  0.19103892]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
